{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔧 Setup\n",
    "\n",
    "The script begins by setting paths to input CSV files (countries, data, and indicator metadata) and defining constants like the minimum required year and number of top countries to consider.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Indicator Stacks\n",
    "\n",
    "Indicators are grouped into four thematic categories (called \"stacks\"):\n",
    "\n",
    "* **Tertiary Demand**: Measures related to higher education participation (e.g. enrolment, graduation).\n",
    "* **Digital Access**: Metrics around internet and computer access.\n",
    "* **Demographics**: Focuses on the youth population, a key target for online education.\n",
    "* **Purchasing Power**: Economic indicators such as GDP and GNI per capita.\n",
    "\n",
    "Each stack is defined by a set of keywords used to identify relevant indicators.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Indicator Extraction\n",
    "\n",
    "The script loads the indicator metadata and searches for matches based on the defined keywords. It optionally includes topic-level filters for stacks like \"tertiary demand.\"\n",
    "\n",
    "The result is a curated list of indicator codes and names for each stack, to be used in later analysis."
   ],
   "id": "b88eb3778dcab69b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0. Paths & parameters\n",
    "# ------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "DATA_DIR   = Path(\"./Projet+Python_Dataset_Edstats_csv\")\n",
    "COUNTRIES  = DATA_DIR / \"EdStatsCountry.csv\"\n",
    "DATA       = DATA_DIR / \"EdStatsData.csv\"\n",
    "INDICATORS = DATA_DIR / \"EdStatsSeries.csv\"\n",
    "\n",
    "MIN_REQUIRED = 2000\n",
    "TOP_N        = 50\n",
    "\n",
    "# 1. The four keyword stacks\n",
    "STACKS = {\n",
    "    \"tertiary_demand\": [                         # (a) demand / participation\n",
    "        r\"\\bTertiary\\b\", r\"Post[- ]Secondary\", r\"\\bISCED 4\\b\",\n",
    "        r\"Gross enrolment\", r\"Graduat\", r\"Completion\",\n",
    "    ],\n",
    "    \"digital_access\": [                          # (b) digital access & ICT\n",
    "        r\"Internet users\", r\"Personal computers\",\n",
    "    ],\n",
    "    \"demographics\": [                            # (c) demographics of learners\n",
    "        r\"Population, ages 15-24\",\n",
    "    ],\n",
    "    \"purchasing_power\": [                        # (d) purchasing power, macro\n",
    "        r\"GDP per capita\", r\"GNI per capita\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "TOPIC_EXTRA = {                                  # topic strings that matter\n",
    "    \"tertiary_demand\": r\"Tertiary|Post-Secondary/Non-Tertiary\",\n",
    "}\n",
    "\n",
    "# 2. Build the hit-lists for each stack\n",
    "series_df = pd.read_csv(INDICATORS, low_memory=False)\n",
    "\n",
    "stack_hits = {}\n",
    "\n",
    "for stack, kw_list in STACKS.items():\n",
    "    pat_name   = \"|\".join(kw_list)\n",
    "    mask_name  = series_df[\"Indicator Name\"].str.contains(\n",
    "        pat_name, case=False, regex=True, na=False\n",
    "    )\n",
    "    if stack in TOPIC_EXTRA:\n",
    "        mask_topic = series_df[\"Topic\"].str.contains(\n",
    "            TOPIC_EXTRA[stack], case=False, regex=True, na=False\n",
    "        )\n",
    "        mask = mask_name | mask_topic\n",
    "    else:\n",
    "        mask = mask_name\n",
    "\n",
    "    hits = (\n",
    "        series_df.loc[mask, [\"Series Code\", \"Indicator Name\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    stack_hits[stack] = hits"
   ],
   "id": "998342bca19974a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 Indicator Coverage Analysis & Filtering\n",
    "\n",
    "This section of the script ensures that only well-populated indicators are kept for further analysis by counting and filtering based on data availability.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 Step 3: Count Available Data Points per Indicator\n",
    "\n",
    "The script loads the full dataset and counts how many actual (non-missing) yearly values exist for each row. It then aggregates these counts by indicator to compute the **total number of valid data points per indicator**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 4: Select High-Coverage Indicators\n",
    "\n",
    "For each indicator stack (e.g., demand, access, demographics), the script retains only the indicators that meet a minimum threshold of data availability (`MIN_REQUIRED`). This helps ensure reliability and comparability across countries.\n",
    "\n",
    "A summary is printed showing:\n",
    "\n",
    "* How many indicators were evaluated.\n",
    "* How many met the threshold.\n",
    "* The retention percentage.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧹 Step 5: Filter the Full Dataset\n",
    "\n",
    "Finally, the script filters the full data table to keep **only the high-coverage indicators** selected in the previous step. This results in a cleaned dataset (`data_strong`) focused on meaningful and complete indicators, ready for scoring or visualization."
   ],
   "id": "5f690d5d217aa9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count real data cells for every indicator\n",
    "data_df = pd.read_csv(DATA, low_memory=False)\n",
    "YEAR_COLS = data_df.columns[4:]\n",
    "\n",
    "data_df[\"n_values_row\"] = data_df[YEAR_COLS].notna().sum(axis=1)\n",
    "\n",
    "# Function to roll counts up per indicator\n",
    "def get_indicator_counts(codes: pd.Series) -> pd.DataFrame:\n",
    "    tmp = data_df[data_df[\"Indicator Code\"].isin(codes)]\n",
    "    counts = (\n",
    "        tmp.groupby([\"Indicator Code\", \"Indicator Name\"], as_index=False)\n",
    "        [\"n_values_row\"].sum()\n",
    "        .rename(columns={\"n_values_row\": \"total_values\"})\n",
    "        .sort_values(\"total_values\", ascending=False)\n",
    "    )\n",
    "    return counts\n",
    "\n",
    "stack_counts = {\n",
    "    stack: get_indicator_counts(hits[\"Series Code\"])\n",
    "    for stack, hits in stack_hits.items()\n",
    "}\n",
    "\n",
    "# 4. Decide which indicators to keep per stack\n",
    "stack_best = {}\n",
    "\n",
    "for stack, counts in stack_counts.items():\n",
    "    if MIN_REQUIRED:\n",
    "        keep = counts[counts[\"total_values\"] >= MIN_REQUIRED]\n",
    "    else:\n",
    "        keep = counts.head(TOP_N)\n",
    "    stack_best[stack] = keep\n",
    "\n",
    "    print(f\"\\n=== {stack.replace('_', ' ').title()} ===\")\n",
    "    print(keep.to_string(index=False))\n",
    "    print(\n",
    "        f\"Kept {len(keep):,} of {len(counts):,} indicators \"\n",
    "        f\"({len(keep)/max(len(counts),1):.1%}) with \"\n",
    "        f\"{'≥'+str(MIN_REQUIRED) if MIN_REQUIRED else 'Top-'+str(TOP_N)} values.\"\n",
    "    )\n",
    "\n",
    "# 5. Filter the big data table to the kept indicators only\n",
    "kept_codes = pd.concat([df[\"Indicator Code\"] for df in stack_best.values()]).unique()\n",
    "data_strong = data_df[data_df[\"Indicator Code\"].isin(kept_codes)].copy()\n"
   ],
   "id": "cc8f000453019e95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ✅ Step 6: Final Indicator Shortlist\n",
    "\n",
    "This step manually defines a curated list of key indicators to be used in the final analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Purpose\n",
    "\n",
    "From the previously filtered high-coverage indicators, a small set of the **most relevant and interpretable indicators** is selected for each thematic category. These were chosen for their clarity, availability, and direct relevance to assessing the potential for online post-secondary education expansion.\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 Final Indicator Groups\n",
    "\n",
    "* **Tertiary Demand**\n",
    "  Indicators measuring enrolment levels and the size of the post-secondary age group.\n",
    "\n",
    "* **Digital Access**\n",
    "  A core metric reflecting internet penetration among the population.\n",
    "\n",
    "* **Demographics**\n",
    "  Focused on the size of the youth population (ages 15–24), the primary target audience.\n",
    "\n",
    "* **Purchasing Power**\n",
    "  Includes GDP and GNI per capita, representing economic capability at the individual level."
   ],
   "id": "27405f4e4b52d0c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Define the final indicator short-list\n",
    "INDICATOR_CANDIDATES = {\n",
    "    \"tertiary_demand\": [\n",
    "        \"SE.TER.ENRR\",       # Gross enrolment ratio, tertiary (%)\n",
    "        \"SP.TER.TOTL.IN\",    # Population of tertiary-age cohort (number)\n",
    "        \"UIS.TE_100000.56\",  # Tertiary enrolment per 100 000 population\n",
    "    ],\n",
    "    \"digital_access\": [\n",
    "        \"IT.NET.USER.P2\",    # Internet users per 100 people\n",
    "    ],\n",
    "    \"demographics\": [\n",
    "        \"SP.POP.1524.TO.UN\", # Population aged 15-24 (total)\n",
    "    ],\n",
    "    \"purchasing_power\": [\n",
    "        \"NY.GDP.PCAP.PP.KD\", # GDP pc, PPP, constant 2011 $\n",
    "        \"NY.GNP.PCAP.CD\",    # GNI pc, Atlas method, current $\n",
    "    ],\n",
    "}"
   ],
   "id": "371e2119d401d2ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🧭 Step 7: Visualize Missing Data\n",
    "\n",
    "This step generates a heatmap to provide a **visual overview of missing values** in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Purpose\n",
    "\n",
    "By plotting a missing-value matrix for the entire dataset, this heatmap helps quickly identify:\n",
    "\n",
    "* Patterns or clusters of missing data across years or indicators.\n",
    "* Indicators or countries with sparse data.\n",
    "* Potential structural gaps (e.g., certain years consistently missing).\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Output\n",
    "\n",
    "The heatmap displays rows as data entries and columns as features. Missing values are highlighted, making data quality issues easy to spot before analysis continues."
   ],
   "id": "ef219b4512e5a808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Display heatmap missing values\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(data_df.isnull(), cbar=False,\n",
    "            yticklabels=False)\n",
    "plt.title(\"Missing-value map — EdStatsData\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "290984e58a3bbf65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📥 Step 8: Load and Filter Final Data\n",
    "\n",
    "The script reads the main education dataset in chunks, keeping only:\n",
    "\n",
    "* The selected shortlist of high-quality indicators.\n",
    "* Years of interest: historical (2000–2015) and forecast (2020–2040).\n",
    "* Countries listed in the metadata.\n",
    "\n",
    "The result is a cleaned dataset (`raw`) containing only the relevant rows and columns for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Step 9: Extract Most Recent Values\n",
    "\n",
    "For each indicator concept (e.g. digital access, demographics), the script:\n",
    "\n",
    "* Selects the most complete indicator (best coverage).\n",
    "* Uses the latest available **historical value**, or falls back to the **earliest forecast** if needed.\n",
    "* Flags whether the value came from a forecast.\n",
    "\n",
    "The result is a tidy table (`tidy`) with one row per country per concept.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Step 10: Collapse to One Value per Concept\n",
    "\n",
    "The script reshapes the tidy table into a wide format with one row per country and one column per concept. It also merges country metadata like region and income group.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Step 11: Score Normalization & Composite Index\n",
    "\n",
    "Each concept is normalized using min-max scaling:\n",
    "\n",
    "* Higher scores = better conditions (e.g. large youth population, strong digital access).\n",
    "* For **enrolment**, lower values imply higher market gaps (and opportunity), so the score is reversed.\n",
    "\n",
    "A weighted formula then combines these into a single **potential score**, indicating each country’s suitability for online post-secondary education expansion.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏁 Step 12: Ranking & Output\n",
    "\n",
    "Countries are ranked by their potential score and saved to a CSV file for further use. This final ranked list identifies the most promising markets based on data coverage, need, and infrastructure readiness."
   ],
   "id": "c4089b4a9d8b571f"
  },
  {
   "cell_type": "code",
   "id": "bce1d14b-33ed-45d9-98c7-057fa8f0f601",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 8. Read & filter data\n",
    "YEAR_MIN, YEAR_MAX  = 2000, 2015\n",
    "FCAST_MIN, FCAST_MAX = 2020, 2040\n",
    "\n",
    "codes_flat = {code for lst in INDICATOR_CANDIDATES.values() for code in lst}\n",
    "years_keep = {str(y) for y in range(YEAR_MIN,  YEAR_MAX+1)} | \\\n",
    "             {str(y) for y in range(FCAST_MIN, FCAST_MAX+1)}\n",
    "\n",
    "keep_cols = lambda c: (\n",
    "        c in [\"Country Code\", \"Indicator Code\"]\n",
    "        or (c.isdigit() and c in years_keep)\n",
    ")\n",
    "\n",
    "countries   = pd.read_csv(COUNTRIES, low_memory=False)\n",
    "country_set = set(countries[\"Country Code\"])\n",
    "\n",
    "parts = []\n",
    "for chunk in pd.read_csv(DATA, chunksize=30_000, usecols=keep_cols, low_memory=False):\n",
    "    parts.append(\n",
    "        chunk[\n",
    "            chunk[\"Indicator Code\"].isin(codes_flat)\n",
    "            & chunk[\"Country Code\"].isin(country_set)\n",
    "            ]\n",
    "    )\n",
    "raw = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "year_cols  = raw.columns[raw.columns.str.isdigit()]\n",
    "\n",
    "hist_cols  = [c for c in year_cols if YEAR_MIN  <= int(c) <= YEAR_MAX]\n",
    "fcast_cols = [c for c in year_cols if FCAST_MIN <= int(c) <= FCAST_MAX]\n",
    "\n",
    "# 9. Helperlatest observed or earliest forecast\n",
    "def latest_or_forecast(df_hist, df_fcast):\n",
    "    hist = (df_hist.reindex(sorted(df_hist.columns, key=int, reverse=True), axis=1)\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")\n",
    "            .bfill(axis=1).iloc[:, 0])\n",
    "\n",
    "    if df_fcast.shape[1]:\n",
    "        fcast = (df_fcast.reindex(sorted(df_fcast.columns, key=int), axis=1)\n",
    "                 .apply(pd.to_numeric, errors=\"coerce\")\n",
    "                 .ffill(axis=1).iloc[:, -1])\n",
    "    else:\n",
    "        fcast = pd.Series(np.nan, index=df_hist.index)\n",
    "\n",
    "    value = hist.fillna(fcast)\n",
    "    flag  = (hist.isna() & value.notna()).astype(int)\n",
    "    return value, flag\n",
    "\n",
    "# 10. Collapse to one best indicator per concept\n",
    "tidy_blocks = []\n",
    "\n",
    "for concept, codes in INDICATOR_CANDIDATES.items():\n",
    "    coverage = {\n",
    "        code: raw.loc[raw[\"Indicator Code\"] == code, hist_cols].notna().any(axis=1).sum()\n",
    "        for code in codes\n",
    "    }\n",
    "    best_code = max(coverage, key=coverage.get)\n",
    "\n",
    "    block = raw[raw[\"Indicator Code\"] == best_code].set_index(\"Country Code\")\n",
    "    val, flag = latest_or_forecast(block[hist_cols], block[fcast_cols])\n",
    "\n",
    "    tidy_blocks.append(\n",
    "        pd.DataFrame({\n",
    "            \"Country Code\": block.index,\n",
    "            \"concept\"     : concept,\n",
    "            \"value\"       : val.values,\n",
    "            \"forecast\"    : flag.values,\n",
    "        })\n",
    "    )\n",
    "\n",
    "tidy = pd.concat(tidy_blocks, ignore_index=True)\n",
    "\n",
    "# 11. Wide table  +  composite score\n",
    "wide = (tidy.pivot(index=\"Country Code\", columns=\"concept\", values=\"value\")\n",
    "        .reset_index()\n",
    "        .merge(countries, on=\"Country Code\", how=\"left\"))\n",
    "\n",
    "# ─── rescale helpers ──────────────────────────────────────────────\n",
    "def minmax(s):\n",
    "    v = s.dropna()\n",
    "    if v.empty: return pd.Series(np.nan, index=s.index)\n",
    "    lo, hi = v.min(), v.max()\n",
    "    if lo == hi: return pd.Series(0.5, index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "# individual scores\n",
    "wide[\"size_score\"]     = minmax(wide[\"demographics\"])\n",
    "wide[\"access_score\"]   = minmax(wide[\"digital_access\"])\n",
    "tertiary_scaled        = minmax(wide[\"tertiary_demand\"])\n",
    "wide[\"gap_score\"]      = 1 - tertiary_scaled          # low enrolment ⇒ big gap\n",
    "wide[\"wealth_score\"]   = minmax(wide[\"purchasing_power\"])\n",
    "\n",
    "weights = dict(size=0.30, access=0.25, gap=0.25, wealth=0.20)\n",
    "\n",
    "wide[\"potential_score\"] = (\n",
    "        weights[\"size\"]   * wide[\"size_score\"]\n",
    "        + weights[\"access\"] * wide[\"access_score\"]\n",
    "        + weights[\"gap\"]    * wide[\"gap_score\"]\n",
    "        + weights[\"wealth\"] * wide[\"wealth_score\"]\n",
    ")\n",
    "\n",
    "# 12. Rank & plot\n",
    "ranking = (\n",
    "    wide[[\"Country Code\", \"Short Name\", \"Region\", \"Income Group\",\n",
    "          \"potential_score\"]]\n",
    "    .dropna(subset=[\"potential_score\"])\n",
    "    .sort_values(\"potential_score\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "ranking.to_csv(\"country_potential_scores_final.csv\", index=False)\n",
    "print(f\"✅ Saved {ranking.shape[0]} ranked countries to CSV\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 Visualization & Insights\n",
    "\n",
    "This section visualizes the final results of the scoring model, helping interpret the rankings and understand the underlying drivers of each country’s potential for online school expansion.\n",
    "\n",
    "---\n",
    "\n",
    "### 🏆 1. Top Countries Bar Chart\n",
    "\n",
    "Displays the top 20 countries ranked by **potential score** in a horizontal bar chart.\n",
    "This provides a quick visual of the highest-scoring markets for online education expansion.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 2. Score Component Correlation Heatmap\n",
    "\n",
    "A correlation matrix shows how the different **score components** relate to one another (e.g., whether internet access is correlated with purchasing power).\n",
    "This helps identify dependencies or redundancies in the scoring dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗺️ 3. World Map: Geospatial Distribution\n",
    "\n",
    "A choropleth map visualizes the **geographic distribution** of potential scores.\n",
    "Top-ranking countries are annotated directly on the map, highlighting regions with the strongest opportunity signals.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 4. Score Breakdown: Normalized vs Raw Values\n",
    "\n",
    "This dual-panel visualization shows:\n",
    "\n",
    "* **Left Panel:** Normalized component scores (0–1 scale) for each top country, illustrating how each dimension contributes to the final potential score.\n",
    "* **Right Panel:** Corresponding **raw indicator values** (on a log scale) to understand the actual values behind the scores, correcting for skewed distributions.\n",
    "\n",
    "Together, these plots offer a comprehensive view of both **who ranks highest** and **why**."
   ],
   "id": "170eabe026152190"
  },
  {
   "cell_type": "code",
   "id": "d946cbc4-3e53-4bb7-a67d-8964ff168f8b",
   "metadata": {},
   "source": [
    "TOP_N = 20\n",
    "ranking = wide[[\"Country Code\", \"Short Name\", \"Region\", \"Income Group\", \"potential_score\"]].copy()\n",
    "ranking = ranking.dropna(subset=[\"potential_score\"]).sort_values(\"potential_score\", ascending=False)\n",
    "\n",
    "# 1. Top 20 Countries Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "topN = ranking.head(TOP_N)\n",
    "sns.barplot(data=topN, x=\"potential_score\", y=\"Short Name\", hue=\"Short Name\", dodge=False, legend=False)\n",
    "plt.title(\"Top 20 Countries for Online School Expansion\")\n",
    "plt.xlabel(\"Potential Score\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Score Correlation Heatmap\n",
    "score_cols = [\"size_score\", \"access_score\", \"gap_score\", \"wealth_score\", \"potential_score\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(wide[score_cols].corr(), annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Between Score Components\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. World Geo Map of Potential Score\n",
    "world = gpd.read_file(\"./ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\n",
    "world = world.rename(columns={\"NAME\": \"Short Name\"})\n",
    "world_data = world.merge(ranking, on=\"Short Name\", how=\"left\")\n",
    "\n",
    "top_countries = ranking.sort_values(\"potential_score\", ascending=False).head(TOP_N)[\"Short Name\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "world_data.plot(\n",
    "    column=\"potential_score\",\n",
    "    ax=ax,\n",
    "    cmap=\"YlGnBu\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"label\": \"Potential Score\", \"shrink\": 0.5},\n",
    "    edgecolor=\"gray\",\n",
    "    missing_kwds={\"color\": \"lightgray\", \"label\": \"No data\"}\n",
    ")\n",
    "ax.set_title(f\"Top {TOP_N} Countries for Online School Expansion (Age 17+)\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Annotate only the top countries\n",
    "for _, row in world_data[world_data[\"Short Name\"].isin(top_countries)].iterrows():\n",
    "    try:\n",
    "        centroid = row[\"geometry\"].centroid\n",
    "        ax.text(centroid.x, centroid.y, row[\"Short Name\"], fontsize=6,\n",
    "                ha='center', va='center', color='black')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 4. Score Breakdown – Top N Countries\n",
    "components = [\"size_score\", \"access_score\", \"gap_score\", \"wealth_score\"]\n",
    "raws = [\"demographics\", \"digital_access\", \"tertiary_demand\", \"purchasing_power\"]\n",
    "component_labels = [\"Youth Pop (score)\", \"Internet (score)\", \"Enrolment Gap (score)\", \"Wealth (score)\"]\n",
    "raw_labels = [\"Youth Pop (raw)\", \"Internet (raw)\", \"Enrolment (%)\", \"GDP/GNI\"]\n",
    "\n",
    "top10 = wide[wide[\"Country Code\"].isin(ranking.head(TOP_N)[\"Country Code\"])].copy()\n",
    "melted_scores = top10.melt(id_vars=[\"Short Name\"], value_vars=components, var_name=\"Component\", value_name=\"Score\")\n",
    "melted_scores[\"Component\"] = melted_scores[\"Component\"].map(dict(zip(components, component_labels)))\n",
    "\n",
    "melted_raws = top10.melt(id_vars=[\"Short Name\"], value_vars=raws, var_name=\"Component\", value_name=\"Raw Value\")\n",
    "melted_raws[\"Component\"] = melted_raws[\"Component\"].map(dict(zip(raws, raw_labels)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8), gridspec_kw={\"width_ratios\": [1.2, 1]})\n",
    "\n",
    "# Normalized score barplot\n",
    "sns.barplot(data=melted_scores, x=\"Score\", y=\"Short Name\", hue=\"Component\", ax=axes[0])\n",
    "axes[0].set_title(\"Score Component Breakdown (Normalized)\")\n",
    "axes[0].set_xlabel(\"Score (0–1)\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Raw values barplot (use log scale)\n",
    "sns.barplot(data=melted_raws, x=\"Raw Value\", y=\"Short Name\", hue=\"Component\", ax=axes[1])\n",
    "axes[1].set_xscale(\"log\")  # Apply log scale to fix range imbalance\n",
    "axes[1].set_title(\"Raw Indicator Values (Log Scale)\")\n",
    "axes[1].set_xlabel(\"Raw Value (log scale)\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "axes[1].xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
